{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "private_outputs": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN5hX3dig1p0ZD/rNNmDknA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jermwatt/morphi_lab/blob/collab_demos/scratch_notebooks/2_video_pipeline_unoptimized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.  Setup"
      ],
      "metadata": {
        "id": "kVbKjgHovhAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "XZEkR12-eG-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iHfZS44dDgY"
      },
      "outputs": [],
      "source": [
        "# install reqiuredlibraries \n",
        "!pip install \"ultralytics==8.0.111\" \"transformers==4.29.2\" \"timm==0.9.2\" \"diffusers==0.16.1\" \"safetensors==0.3.1\" \"accelerate==0.19.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from ultralytics import YOLO\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm \n",
        "import torch\n",
        "from diffusers import StableDiffusionInpaintPipeline\n",
        "import diffusers\n",
        "diffusers.logging.set_verbosity_error()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "parent_dir = os.getcwd()"
      ],
      "metadata": {
        "id": "O6KHThW77BGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pull yolo model to this macine\n",
        "!curl -sSL -o /content/YOLOv8m-seg.pt https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-seg.pt"
      ],
      "metadata": {
        "id": "xT2g8usBvnLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "diffusion_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
        "     torch_dtype=torch.float16).to(device)"
      ],
      "metadata": {
        "id": "XnXBhwKn_XTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if one wants to set `leave=False`\n",
        "diffusion_pipe.set_progress_bar_config(leave=False)\n",
        "\n",
        "# if one wants to disable `tqdm`\n",
        "diffusion_pipe.set_progress_bar_config(disable=True)"
      ],
      "metadata": {
        "id": "Sgwa4VHKwyn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.  Segmenter"
      ],
      "metadata": {
        "id": "XNNuzL3uvNnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0. Functions"
      ],
      "metadata": {
        "id": "YD0RIzJ32feb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Segmenter:\n",
        "    def __init__(self, conf=0.5):\n",
        "        self.conf = conf\n",
        "        self.model = YOLO(parent_dir + '/YOLOv8m-seg.pt')\n",
        "        self.model.to('cuda')\n",
        "        self.img = None\n",
        "        self.img_height = None\n",
        "        self.img_width = None\n",
        "        self.xmin = None\n",
        "        self.ymin = None\n",
        "        self.xmax = None\n",
        "        self.ymax = None\n",
        "        self.seg = None\n",
        "        self.segmentation_result = None\n",
        "        self.detection_window_path = parent_dir + '/temp/temp.png'\n",
        "\n",
        "    def reset(self):\n",
        "        self.img = None\n",
        "        self.xmin = None\n",
        "        self.ymin = None\n",
        "        self.xmax = None\n",
        "        self.ymax = None\n",
        "        self.seg = None\n",
        "        self.width = None\n",
        "        self.height = None\n",
        "        self.segmentation_result = None\n",
        "\n",
        "    def read_img_path(self, img_path):\n",
        "        self.reset()\n",
        "        self.img = cv2.imread(img_path)\n",
        "        h, w, _ = self.img.shape\n",
        "        self.height = h\n",
        "        self.width = w\n",
        "\n",
        "    def read_img(self, img):\n",
        "        self.reset()\n",
        "        self.img = img\n",
        "        h, w, _ = self.img.shape\n",
        "        self.height = h\n",
        "        self.width = w\n",
        "\n",
        "    def segment(self):\n",
        "        self.segmentation_result = self.model.predict(source=self.img,\n",
        "                                                      classes=[39, 41, 67],\n",
        "                                                      conf=self.conf,\n",
        "                                                      show_labels=False,\n",
        "                                                      boxes=False,\n",
        "                                                      verbose=False,\n",
        "                                                      half=True,\n",
        "                                                      max_det=1,\n",
        "                                                      device=0)\n",
        "\n",
        "        # class names\n",
        "        self.class_names = self.model.names\n",
        "\n",
        "        # random colors for plotting\n",
        "        # self.colors = [[random.randint(0, 255) for _ in range(3)] for _ in self.class_names]\n",
        "        self.colors = [[100,0,100] for _ in self.class_names]\n",
        "\n",
        "        # extract segmentation result\n",
        "        h, w, _ = self.img.shape\n",
        "        boxes = self.segmentation_result[0].boxes\n",
        "        masks = self.segmentation_result[0].masks\n",
        "\n",
        "        if masks is not None:\n",
        "            masks = masks.data.cpu()\n",
        "            for seg, box in zip(masks.data.cpu().numpy(), boxes):\n",
        "                seg = cv2.resize(seg, (w, h)).astype(np.uint8)\n",
        "\n",
        "                self.xmin = int(box.data[0][0])\n",
        "                self.ymin = int(box.data[0][1])\n",
        "                self.xmax = int(box.data[0][2])\n",
        "                self.ymax = int(box.data[0][3])\n",
        "                self.seg = seg\n",
        "\n",
        "                self.detection_window = self.img[self.ymin:self.ymax,\n",
        "                                                 self.xmin:self.xmax]\n",
        "\n",
        "                break\n",
        "\n",
        "    def save_segment(self):\n",
        "        cv2.imwrite(self.detection_window_path, self.detection_window)\n",
        "\n",
        "    @staticmethod\n",
        "    def overlay(image, mask, color, alpha, resize=None):\n",
        "        colored_mask = np.expand_dims(mask, 0).repeat(3, axis=0)\n",
        "        colored_mask = np.moveaxis(colored_mask, 0, -1)\n",
        "        masked = np.ma.MaskedArray(image, mask=colored_mask, fill_value=color)\n",
        "        image_overlay = masked.filled()\n",
        "\n",
        "        if resize is not None:\n",
        "            image = cv2.resize(image.transpose(1, 2, 0), resize)\n",
        "            image_overlay = cv2.resize(image_overlay.transpose(1, 2, 0), resize)\n",
        "\n",
        "        image_combined = cv2.addWeighted(image, 1 - alpha, image_overlay, alpha, 0)\n",
        "\n",
        "        return image_combined\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_one_box(x, img, color=None, label=None, line_thickness=3):\n",
        "        # Plots one bounding box on image img\n",
        "        tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
        "        color = color or [random.randint(0, 255) for _ in range(3)]\n",
        "        c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
        "        cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
        "        if label:\n",
        "            tf = max(tl - 1, 1)  # font thickness\n",
        "            t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
        "            c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
        "            cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
        "            cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
        "\n",
        "    def project_segmentations(self, show_overlay=True, show_boxes=False, show_result=False):\n",
        "        # unpack segmentation results\n",
        "        boxes = self.segmentation_result[0].boxes\n",
        "        masks = self.segmentation_result[0].masks\n",
        "\n",
        "        # loop over masks and plot\n",
        "        if masks is not None:\n",
        "            masks = masks.data.cpu()\n",
        "            for seg, box in zip(masks.data.cpu().numpy(), boxes):\n",
        "                seg = cv2.resize(seg, (self.width, self.height)).astype(np.uint8)\n",
        "\n",
        "                if show_overlay:\n",
        "                    self.img = self.overlay(self.img, seg, self.colors[int(box.cls)], 0.4)\n",
        "\n",
        "                xmin = int(box.data[0][0])\n",
        "                ymin = int(box.data[0][1])\n",
        "                xmax = int(box.data[0][2])\n",
        "                ymax = int(box.data[0][3])\n",
        "\n",
        "                if show_boxes:\n",
        "                    self.plot_one_box([xmin, ymin, xmax, ymax],\n",
        "                                      self.img,\n",
        "                                      self.colors[int(box.cls)],\n",
        "                                      f'{self.class_names[int(box.cls)]} {float(box.conf):.3}')\n",
        "\n",
        "    def show_result(self):\n",
        "        # image_rgb = cv2.imshow('img', img)\n",
        "        image_rgb = cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        plt.imshow(image_rgb)\n",
        "        plt.axis('off')  # optional: disable the axis\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "nKCl7-MSuG7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1.  Image test"
      ],
      "metadata": {
        "id": "yyDdOGbYx5RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pull image\n",
        "!curl -sSL -o /content/test_img.png https://github.com/jermwatt/morphi_lab/blob/collab_demos/test_data/test_input/test_img.png?raw=true "
      ],
      "metadata": {
        "id": "W_O1IxCSx6XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path to image \n",
        "img_path = \"/content/test_img.png\"\n",
        "\n",
        "# load in image\n",
        "img = cv2.imread(img_path)\n",
        "\n",
        "# show img\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "img = Image.fromarray(img)\n",
        "img = img.convert('RGB')\n",
        "img"
      ],
      "metadata": {
        "id": "pnPT9Q58z92X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# segment frame\n",
        "segmenter = Segmenter(conf=0.1)\n",
        "segmenter.read_img_path(img_path)\n",
        "segmenter.segment()\n",
        "segmenter.project_segmentations()\n",
        "segmenter.show_result()"
      ],
      "metadata": {
        "id": "KnvYoRhn0NXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert mask to PIL image\n",
        "mask = segmenter.segmentation_result[0].masks.data[0]\n",
        "mask = (mask * 255).to(torch.uint8)\n",
        "mask = torch.stack((mask,) * 3, dim=0)\n",
        "mask_pil = transforms.ToPILImage()(mask)\n",
        "mask_pil"
      ],
      "metadata": {
        "id": "hY6gNDADDCXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Video test"
      ],
      "metadata": {
        "id": "okasdX7Y4hTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pull video\n",
        "!curl -sSL -o /content/test_video.avi https://github.com/jermwatt/morphi_lab/blob/collab_demos/test_data/test_input/1.avi?raw=true "
      ],
      "metadata": {
        "id": "WXPdO94P4TFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_video(input_path, output_path):\n",
        "    # read in video\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "    # get total number of frames\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # setup tqdm\n",
        "    pbar = tqdm(total=total_frames)\n",
        "\n",
        "    # get frame width and height from img\n",
        "    success, img = cap.read()\n",
        "\n",
        "    # get frame width and height from img\n",
        "    frame_width = img.shape[1]\n",
        "    frame_height = img.shape[0]\n",
        "\n",
        "    # define codec and create VideoWriter object\n",
        "    size = (frame_width, frame_height)\n",
        "    result = cv2.VideoWriter(output_path,\n",
        "                             cv2.VideoWriter_fourcc(*'MJPG'),\n",
        "                             30, size)\n",
        "\n",
        "    # loop through video\n",
        "    while success is not None:\n",
        "        try:\n",
        "            # segment frame\n",
        "            segmenter = Segmenter(conf=0.1)\n",
        "            segmenter.read_img(img)\n",
        "            segmenter.segment()\n",
        "            segmenter.project_segmentations()\n",
        "\n",
        "            # write frame\n",
        "            result.write(segmenter.img)\n",
        "        except Exception as e:\n",
        "            print(e, flush=True)\n",
        "            break\n",
        "\n",
        "        # read in next frame\n",
        "        ret, img = cap.read()\n",
        "\n",
        "        # update tqdm\n",
        "        pbar.update(1)\n",
        "\n",
        "    # release video\n",
        "    cap.release()\n",
        "    result.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    # close tqdm\n",
        "    pbar.close()"
      ],
      "metadata": {
        "id": "0qfHrASy0OEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = '/content/test_video.avi'\n",
        "output_path = '/content/test_video_segmented.avi'\n",
        "segment_video(input_path, output_path)"
      ],
      "metadata": {
        "id": "rXHyrAPI54_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.  Diffuse objects"
      ],
      "metadata": {
        "id": "kaXCDucC_BsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0.  Functions"
      ],
      "metadata": {
        "id": "oNoxsGgHAKDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_diffuse_results(segmented_frame,\n",
        "                         mask,\n",
        "                         difused_frame):\n",
        "  # Create a figure with three subplots\n",
        "  fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
        "\n",
        "  # Display the first image\n",
        "  axes[0].imshow(segmented_frame)\n",
        "  axes[0].axis('off')\n",
        "  axes[0].set_title('segmented image')\n",
        "\n",
        "  # Display the second image\n",
        "  axes[1].imshow(mask)\n",
        "  axes[1].axis('off')\n",
        "  axes[1].set_title('mask')\n",
        "\n",
        "  # Display the third image\n",
        "  axes[2].imshow(difused_frame)\n",
        "  axes[2].axis('off')\n",
        "  axes[2].set_title('diffused image')\n",
        "\n",
        "  # Adjust the layout\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Show the figure\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_frame(img_path):\n",
        "    av = AV(img_path)\n",
        "    av.show_result()\n",
        "    \n",
        "def plot_frame_and_mask(img, mask):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    ax[0].imshow(img)\n",
        "    ax[1].imshow(mask)\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "# transform frame\n",
        "def transform_frame(frame,\n",
        "                     prompt=\"coca cola can, high resolution, high quality, logo visible, red and white colors, aluminum\",\n",
        "                     conf=0.1):\n",
        "    # detect and segment\n",
        "    segmenter = Segmenter(conf=conf)\n",
        "    segmenter.read_img(frame)\n",
        "    segmenter.segment()\n",
        "    segmenter.project_segmentations()\n",
        "    segmented_frame = segmenter.img\n",
        "\n",
        "    # convert mask to PIL image\n",
        "    mask = segmenter.segmentation_result[0].masks.data[0].cpu().numpy()\n",
        "    # mask = cv2.resize(mask, (640, 448))\n",
        "    mask = mask.astype(np.uint8)*255\n",
        "    mask = np.stack((mask,)*3, axis=-1)\n",
        "    mask = Image.fromarray(mask)\n",
        "    mask = mask.convert('RGB')\n",
        "    mask = mask.resize((512, 512)) \n",
        "\n",
        "    # mask = segmenter.segmentation_result[0].masks.data[0]\n",
        "    # mask = (mask * 255).to(torch.uint8)\n",
        "    # mask = torch.stack((mask,) * 3, dim=0)\n",
        "    # mask_pil = transforms.ToPILImage()(mask)\n",
        "    # mask = mask.convert('RGB')\n",
        "    # mask = mask.resize((512, 512)) \n",
        "\n",
        "    # resize segmenter.img to 448x640\n",
        "    frame = cv2.resize(segmenter.img, (640, 448))\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame = Image.fromarray(frame)\n",
        "    frame = frame.convert('RGB')\n",
        "    frame = frame.resize((512, 512))\n",
        "\n",
        "    # difuse mask\n",
        "    difused_frame = diffusion_pipe(prompt=prompt,\n",
        "                                   negative_prompt='fingers, dented can, crushed can, bottle, plastic, low quality, low resolution',\n",
        "                                   image=frame,\n",
        "                                   mask_image=mask,\n",
        "                                   num_inference_steps=10,\n",
        "                                   output_type='np.array').images[0]\n",
        "\n",
        "    # resize\n",
        "    difused_frame = cv2.resize(difused_frame, (640, 448))\n",
        "    \n",
        "    # resize frames to 640x448\n",
        "    segmented_frame = cv2.cvtColor(segmented_frame, cv2.COLOR_BGR2RGB)\n",
        "    segmented_frame = Image.fromarray(segmented_frame)\n",
        "    segmented_frame = segmented_frame.resize((640, 448))\n",
        "    mask = mask.resize((640, 448))\n",
        "    # difused_frame = difused_frame.resize((640, 448))\n",
        "    return segmented_frame, mask, difused_frame"
      ],
      "metadata": {
        "id": "6uMmcqq1_VCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.  Images"
      ],
      "metadata": {
        "id": "BCswIM3pAMwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path to image \n",
        "img_path = \"/content/test_img.png\"\n",
        "\n",
        "# load in image\n",
        "img = cv2.imread(img_path)\n",
        "\n",
        "# transform frame\n",
        "difused_frame = transform_frame(img)"
      ],
      "metadata": {
        "id": "xY5hI39YuJ_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = cv2.resize(difused_frame, (640, 448))"
      ],
      "metadata": {
        "id": "aWz3F45KuXes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.shape"
      ],
      "metadata": {
        "id": "orF27ySXuczV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "difused_frame.shape"
      ],
      "metadata": {
        "id": "bz_XdheDuM-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path to image \n",
        "img_path = \"/content/test_img.png\"\n",
        "\n",
        "# load in image\n",
        "img = cv2.imread(img_path)\n",
        "\n",
        "# transform frame\n",
        "segmented_frame, mask, difused_frame = transform_frame(img)\n",
        "\n",
        "# plot results\n",
        "plot_diffuse_results(segmented_frame,mask,difused_frame)"
      ],
      "metadata": {
        "id": "TZOh1VKvALs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oV_CQ5nsCJHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Video"
      ],
      "metadata": {
        "id": "CcZ3WoXwtAtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def diffuse_video(input_path, output_path):\n",
        "    # read in video\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "    # get total number of frames\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # setup tqdm\n",
        "    pbar = tqdm(total=total_frames)\n",
        "\n",
        "    # get frame width and height from img\n",
        "    success, img = cap.read()\n",
        "\n",
        "    # get frame width and height from img\n",
        "    frame_width = img.shape[1]\n",
        "    frame_height = img.shape[0]\n",
        "\n",
        "    # define codec and create VideoWriter object\n",
        "    size = (frame_width, frame_height)\n",
        "    result = cv2.VideoWriter(output_path,\n",
        "                             cv2.VideoWriter_fourcc(*'MJPG'),\n",
        "                             30, size)\n",
        "\n",
        "    # loop through video\n",
        "    while success is not None:\n",
        "        try:\n",
        "            # segment frame\n",
        "            segmented_frame, mask, difused_frame = transform_frame(img)\n",
        "            \n",
        "            # resize to input frame shape\n",
        "            difused_frame = cv2.resize(difused_frame, (1920, 1080))\n",
        "\n",
        "            # Convert the image depth to CV_8U\n",
        "            # difused_frame = cv2.convertScaleAbs(difused_frame)\n",
        "            # scale_factor = 65535  # Scaling factor for 16-bit depth\n",
        "            # difused_frame = cv2.convertScaleAbs(difused_frame, alpha=(scale_factor / 255.0))\n",
        "\n",
        "            # write frame\n",
        "            result.write(difused_frame)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e, flush=True)\n",
        "            break\n",
        "\n",
        "        # read in next frame\n",
        "        ret, img = cap.read()\n",
        "\n",
        "        # update tqdm\n",
        "        pbar.update(1)\n",
        "\n",
        "    # release video\n",
        "    cap.release()\n",
        "    result.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    # close tqdm\n",
        "    pbar.close()"
      ],
      "metadata": {
        "id": "AANsyQw0tCDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = '/content/test_video.avi'\n",
        "output_path = '/content/test_video_diffused.avi'\n",
        "diffuse_video(input_path, output_path)"
      ],
      "metadata": {
        "id": "Vls6OXEgtfkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "illDREzVvu_0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}